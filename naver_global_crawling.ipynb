{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeHongPark-96/naver_news_crawling/blob/main/naver_global_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0d854b",
      "metadata": {
        "id": "5c0d854b"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import quote_plus\n",
        "import requests\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "import re\n",
        "from datetime import datetime as dt, timedelta\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from dateutil.parser import parse\n",
        "import pymssql\n",
        "from selenium.webdriver import Chrome, ChromeOptions\n",
        "import lxml\n",
        "import lxml.html\n",
        "import schedule\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99125f6e",
      "metadata": {
        "id": "99125f6e",
        "outputId": "671be2f2-7a42-4d05-a351-4f7758a3c64f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'20210825'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yesterday = (datetime.date.today()-timedelta(1)).strftime('%Y%m%d')\n",
        "yesterday"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d846fb3",
      "metadata": {
        "id": "7d846fb3"
      },
      "source": [
        "### 텍스트 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bcde548",
      "metadata": {
        "id": "8bcde548"
      },
      "outputs": [],
      "source": [
        "def cleansing(text):\n",
        "            text = text.replace('// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}',\"\")\n",
        "        \n",
        "            text = re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', '', text)\n",
        "            text = re.sub(\"[\\n\\t]\", '', text)\n",
        "            text = re.sub(r\"\\[.*\\]|\\s-\\s.*\", \"\", text) # [ ] 와 사이의 문자 제거\n",
        "\n",
        "\n",
        "\n",
        "            text = re.sub(\"(=*([가-힣]{2,4}|[가-힣]{6}) 기자(... [가-힣]{2,3}기자|[^가-힣]|$)=*|로봇 기자)\",r' 기자\\3 ',text) # 000기자 -> 기자로 변환\n",
        "            text = re.sub('(‘|\\\")(.+)(’|\\\")',r\" '\\2' \", text)                     # ' \" ’ -> 로  변환\n",
        "            text = re.sub('【(.+)】',r' [\\1] ', text)                                #【( 등과 같은 괄호 '[ ]' 로 변환\n",
        "            text = re.sub('(http|ftp|https)://((\\w+\\.|\\w+/|\\w+|(\\?|=|\\&)\\w+)+)', ' 주소 ', text)  #url -> '주소' 로 변환\n",
        "            text = re.sub('[0-9]+-[0-9]+|02-[0-9]+-[0-9]+',' 번호 ', text)        #000-0000-0000 -> '번호'로 변환\n",
        "            text = re.sub('TOP [0-9]+|TOP[0-9]+',' 순위', text)                    # TOP 1, TOP 10 -> '순위'로 변환\n",
        "            text = re.sub('[0-9]+:[0-9]+|[0-9]+시 [0-9]분',' 시간 ', text)         # 12:12 , 12시 12 분 -> '시간'로 변환\n",
        "            text = re.sub('코로나19|코로나 19',' 코로나 ', text)                   # 코로나19, 코로나 19 -> '코로나' 로 변환\n",
        "            text = re.sub('(\\[\\w+)(=)(\\w+\\])',r'\\1 \\2 \\3',text )                # 해럴드경제(대전)=홍길동기자 ->해럴드경제 (대전) 홍길동기자        \n",
        "            text = re.sub(\"(\\w+)(\\\")(\\w+)\",r\"\\1 \\2 \\3\",text)                    \n",
        "            text = re.sub('(&nbsp;)|(&nbsp)|(&rsquo;)|(&rsquo)|(&lsquo;)|(&lsquo)|(&middot;)|(&middot)|(&#[0-9]+;)|(fffff;)|\\x03|\\n',\"\",text) # css 공통 문법 제거\n",
        "            text = re.sub('(<([^>]+)>)', '', text)                              # html 문법 제거하기 태그포함\n",
        "            text = re.sub(\"[']\", '', text)\n",
        "            text = re.sub('quot|rdquo|ldquo', '\"', text)\n",
        "            text = re.sub('[-=+,#/▶▲●◆■◀\\?.:^$@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》☞△&·]', ' ', text) # 특문제거\n",
        "            re.findall('\\d+', text)\n",
        "\n",
        "            if re.search('[a-zA-Z]+-[a-zA-Z]+(:|;)',text)!=None:\n",
        "                text = \" \".join(re.findall('[가-힣]+|[0-9]+[가-힣]+',text)) #css 문법(font-family 등)이 있는 경우에는 한글/숫자+한글인 것만 추출\n",
        "            \n",
        "            text = re.sub('\\;', \"\", text)\n",
        "            text = re.sub('[-_+]|돋움|&[a-zA-Z]{2,100};',\" \",text)          # 돋움이라는 font가 들어있는 경우는 css에서 제거하기 힘들어서 별도로 제거 + &quot같은 것들도 제거\n",
        "            text = re.sub('(\\w+)(\\s\\.\\.+|\\.\\.+)',\"\",text)     # ... , ..  -> 공백으로 변환\n",
        "            text = re.sub('\\s\\s+',\" \",text)                                # 공백 2개이상 -> 한개로\n",
        "\n",
        "            if re.search('\\S',text)==None:                                          # 텍스트가 비어있으면 '빈칸'로 채움\n",
        "                text = 'blank' \n",
        "\n",
        "            return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c16793b",
      "metadata": {
        "id": "7c16793b"
      },
      "outputs": [],
      "source": [
        "# 어제자 뉴스 크롤링\n",
        "\n",
        "def global_news_crawling(date=yesterday):\n",
        "\n",
        "    # 크롬 옵션\n",
        "    op = ChromeOptions()\n",
        "    op.add_argument('--headless')\n",
        "    op.add_argument('--no-sandbox')\n",
        "    op.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    browser = Chrome('C:/Users/user/Desktop/News_Crawling/chromedriver_win32/chromedriver.exe', options=op)\n",
        "    ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'\n",
        "    \n",
        "    url = \"https://news.naver.com/main/list.naver\"\n",
        "\n",
        "    header ={\n",
        "\n",
        "        # 브라우저에 맞게 수정\n",
        "\n",
        "        # \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
        "        # \"accept-encoding\": \"gzip, deflate, br\",\n",
        "        # \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "        # \"cache-control\": \"max-age=0\",\n",
        "        # \"sec-ch-ua-mobile\": \"?0\",\n",
        "        # \"sec-fetch-dest\": \"document\",\n",
        "        # \"sec-fetch-mode\": \"navigate\",\n",
        "        # \"sec-fetch-site\": \"same-origin\",\n",
        "        # \"sec-fetch-user\": \"?1\",\n",
        "        # \"upgrade-insecure-requests\": \"1\",\n",
        "        # \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "\n",
        "    }\n",
        "    \n",
        "    global_info = pd.DataFrame()\n",
        "    \n",
        "    # 경제 부문만\n",
        "    category='글로벌 경제'\n",
        "\n",
        "\n",
        "    # 마지막 페이지 번호 구하기\n",
        "    for first_page in range(1,1000, 10):\n",
        "\n",
        "        # category 마다 다르게 지정해줘야 함\n",
        "        param = {\n",
        "            \"mode\" : \"LS2D\",\n",
        "            \"mid\" : \"shm\",\n",
        "            \"sid2\" : \"262\",\n",
        "            \"sid1\" : \"101\",\n",
        "            \"date\" : f\"{date}\",\n",
        "            \"page\" : f\"{first_page}\"\n",
        "        }\n",
        "\n",
        "\n",
        "        # 들어갈 링크 가져오기\n",
        "        response = requests.get(url,params=param, headers=header).text\n",
        "        soup = BeautifulSoup(response, 'html.parser')\n",
        "\n",
        "        # \"다음\" 버튼이 있는지 없는지로 판단\n",
        "        pre_btn = soup.find('a', class_='pre nclicks(fls.page)')\n",
        "        next_btn = soup.find('a', class_=\"next nclicks(fls.page)\")\n",
        "\n",
        "        if next_btn != None:\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                last_page = int(soup.find_all(\"a\", class_=\"nclicks(fls.page)\")[-1].text)\n",
        "                print(f\"{category} {date} 일자 최종 페이지 번호는 {last_page}입니다\")                \n",
        "                break\n",
        "\n",
        "            except:\n",
        "                paging_div = soup.find('div', class_='paging')\n",
        "                last_page = int(paging_div.find_all('strong')[-1].text)\n",
        "                print(f\"{category} {date} 일자 최종 페이지 번호는 {last_page}입니다\")                \n",
        "                break\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    # 마지막 페이지까지 기사별 링크 가져오기\n",
        "    all_news_links = []\n",
        "\n",
        "    for page in range(1, last_page+1): #last_page+1\n",
        "\n",
        "        param = {\n",
        "            \"mode\" : \"LS2D\",\n",
        "            \"mid\" : \"shm\",\n",
        "            \"sid1\" : \"101\",\n",
        "            \"sid2\" : \"262\",\n",
        "            \"date\" : f\"{date}\",\n",
        "            \"page\" : f\"{page}\"\n",
        "        }\n",
        "\n",
        "        # 아무런 기사가 없는 날에도 page는 1로 존재하는 경우에 대한 처리\n",
        "\n",
        "        try:\n",
        "            # 들어갈 링크 가져오기\n",
        "            response = requests.get(url,params=param, headers=header).text\n",
        "            soup = BeautifulSoup(response, 'html.parser')\n",
        "\n",
        "\n",
        "            # 이미지가 있는 경우 하나의 기사당 모두 3개의 동일한 href를 가짐 - class명 등 구분 방법이 따로 없어 모두 가져온 후 set() 처리\n",
        "            article_table = soup.find(\"div\", class_=\"list_body newsflash_body\") # link명이 기사용과 다름\n",
        "            page_news_links = article_table.find_all(\"a\")\n",
        "\n",
        "\n",
        "            for page_news_link in page_news_links:\n",
        "                all_news_links.append(page_news_link.attrs['href'])\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except:\n",
        "            break\n",
        "\n",
        "    all_news_links = set(all_news_links)\n",
        "\n",
        "    print(\"기사별 링크 크롤링 완료\")\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "    # 실제 기사 속 내용 모두 가져오기\n",
        "\n",
        "\n",
        "    for i, news_link in enumerate(tqdm(all_news_links)) :\n",
        "\n",
        "        # 기사 속 내용 가져오기\n",
        "        article_response = requests.get(news_link, headers=header).text\n",
        "        article_soup = BeautifulSoup(article_response, 'html.parser')\n",
        "        \n",
        "        browser.get(news_link)\n",
        "        time.sleep(0.5)\n",
        "        \n",
        "        html = browser.page_source\n",
        "        root = lxml.html.fromstring(html)\n",
        "        \n",
        "\n",
        "        # 잘못된 url이 있는 경우가 존재했음 - pass로 제외하고 진행\n",
        "        try:               \n",
        "            news_title = article_soup.find('h3', id='articleTitle').text\n",
        "            news_title = re.sub(r\"\\[.*\\]|\\s-\\s.*\", \"\", news_title)\n",
        "\n",
        "            news_date = article_soup.find('span', class_='t11').text\n",
        "\n",
        "            url_detail = article_soup.find('a', class_='btn_artialoriginal').attrs['href']\n",
        "            contents = article_soup.find('div', class_=\"_article_body_contents\").text\n",
        "    #                     article = re.sub(\"[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\n\\t]\", '', article_soup.find('div', class_=\"_article_body_contents\").text)\n",
        "    #                     article = re.sub(' +', ' ', article).lstrip()\n",
        "            contents = contents.replace('// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}',\"\")\n",
        "\n",
        "        # 좋아요 / 화나요 크롤링 코드 추가\n",
        "            good_body = root.cssselect('#spiLayer > div._reactionModule.u_likeit > ul > li.u_likeit_list.good > a > span.u_likeit_list_count._count')\n",
        "            good = good_body[0].text_content()\n",
        "            bad_body = root.cssselect('#spiLayer > div._reactionModule.u_likeit > ul > li.u_likeit_list.angry > a > span.u_likeit_list_count._count')\n",
        "            bad = bad_body[0].text_content()\n",
        "        \n",
        "\n",
        "            global_info = global_info.append(pd.DataFrame([[category, news_title, news_date, url_detail, contents, good, bad]], columns=['CATEGORY', 'TITLE', 'NEWS_DATE', 'URL_DETAIL', 'clean_content', 'good', 'bad'])).reset_index(drop=True)\n",
        "            time.sleep(0.5)\n",
        "            # article_info = article_info.append(pd.DataFrame([[good, bad]], columns=['good', 'bad'])).reset_index(drop=True)\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # 형식 추가 (날짜 형식 수정 + 날짜별 내림차순 + 날짜와 순번을 결합한 IDX 추가)\n",
        "    global_info['NEWS_DATE'] = global_info['NEWS_DATE'].apply(lambda x: x.replace('오후','PM').replace('오전','AM'))\n",
        "    global_info['NEWS_DATE'] = global_info['NEWS_DATE'].apply(lambda x: datetime.datetime.strptime(x,\"%Y.%m.%d. %p %I:%M\"))\n",
        "\n",
        "\n",
        "\n",
        "    global_info = global_info.sort_values('NEWS_DATE',ascending=False).reset_index(drop=True)\n",
        "    global_info['IDX'] = [date.strftime('%y%m%d') + '{0:04d}'.format(i+1) for i,date in enumerate(global_info['NEWS_DATE'])]\n",
        "    global_info['clean_content'] = global_info['clean_content'].apply(lambda x: cleansing(x))\n",
        "\n",
        "    global_info = global_info[['IDX','CATEGORY','NEWS_DATE','TITLE','clean_content','URL_DETAIL','good', 'bad']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    global_info.to_csv(f'C:/Users/user/Desktop/News_Crawling/global_article/global_info_{date}.csv', encoding='utf-8-sig', index=False)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20df4802",
      "metadata": {
        "id": "20df4802",
        "outputId": "c5da68eb-368b-4b21-95af-9a6f2c255003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "글로벌 경제 20210825 일자 최종 페이지 번호는 6입니다\n",
            "기사별 링크 크롤링 완료\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [03:15<00:00,  1.66s/it]\n"
          ]
        }
      ],
      "source": [
        "global_news_crawling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542eb054",
      "metadata": {
        "id": "542eb054",
        "outputId": "616c8b31-7217-45be-b93f-78207d5ee46c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 118 entries, 0 to 117\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   IDX            118 non-null    int64 \n",
            " 1   CATEGORY       118 non-null    object\n",
            " 2   NEWS_DATE      118 non-null    object\n",
            " 3   TITLE          118 non-null    object\n",
            " 4   clean_content  118 non-null    object\n",
            " 5   URL_DETAIL     118 non-null    object\n",
            " 6   good           118 non-null    int64 \n",
            " 7   bad            118 non-null    int64 \n",
            "dtypes: int64(3), object(5)\n",
            "memory usage: 7.5+ KB\n"
          ]
        }
      ],
      "source": [
        "articles = pd.read_csv(f\"C:/Users/user/Desktop/News_Crawling/global_article/global_info_{yesterday}.csv\")\n",
        "articles['clean_content'] = articles['clean_content'].apply(lambda x: cleansing(x))\n",
        "articles.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533f3947",
      "metadata": {
        "id": "533f3947",
        "outputId": "cc819e81-ba49-40f7-984b-04041ae9b3a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 118 entries, 0 to 117\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   IDX            118 non-null    int64 \n",
            " 1   CATEGORY       118 non-null    object\n",
            " 2   NEWS_DATE      118 non-null    object\n",
            " 3   TITLE          118 non-null    object\n",
            " 4   clean_content  118 non-null    object\n",
            " 5   URL_DETAIL     118 non-null    object\n",
            " 6   good           118 non-null    int64 \n",
            " 7   bad            118 non-null    int64 \n",
            "dtypes: int64(3), object(5)\n",
            "memory usage: 7.5+ KB\n"
          ]
        }
      ],
      "source": [
        "if articles['good'].dtype=='O' and articles['bad'].dtype=='O':\n",
        "    articles['good'] = articles['good'].apply(lambda x: int(x.replace(',', '')))\n",
        "    articles['bad'] = articles['bad'].apply(lambda x: int(x.replace(',', '')))\n",
        "    \n",
        "elif articles['good'].dtype=='O' and articles['bad'].dtype!='O':\n",
        "    articles['good'] = articles['good'].apply(lambda x: int(x.replace(',', '')))\n",
        "    \n",
        "elif articles['good'].dtype!='O' and articles['bad'].dtype=='O':\n",
        "    articles['bad'] = articles['bad'].apply(lambda x: int(x.replace(',', '')))\n",
        "\n",
        "articles.fillna('', inplace=True)\n",
        "\n",
        "articles.info()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py37",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "naver_global_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}